{"cells":[{"cell_type":"markdown","metadata":{},"source":["# CSS 444 Final Project\n","\n","### Mandy Chen, Mustafa Sabir, Mychael Johnson, Michael Cho"]},{"cell_type":"markdown","metadata":{},"source":["# Baltimore, MD Arrest Records 12/31/2009 - 10/22/2021\n","\n","This dataset is all arrest records in Baltimore, MD from 12/31/2009 to 10/22/2021. We found this dataset on the [Police Data Initiative](https://www.policedatainitiative.org/datasets/) database website, which contains various types of datasets from police jurisdictions across the United States. \n","\n","[Open Baltimore Arrest Dataset](https://data.baltimorecity.gov/datasets/baltimore::arrests/about)"]},{"cell_type":"code","execution_count":1,"metadata":{"id":"4csFcmMFPo_K"},"outputs":[],"source":["# Importing required Python Libraries\n","\n","import pandas as pd\n","from matplotlib import pylab\n","import numpy as np\n","import sklearn\n","from sklearn import linear_model\n","import sklearn.preprocessing as preprocessing\n","import sklearn.metrics as metrics\n","import matplotlib.pyplot as plt\n","from statsmodels.stats import proportion\n","import datetime\n","from itertools import product\n","import csv\n","\n","# Code to display whole dataframe with print\n","pd.set_option(\"display.max_rows\", None, \"display.max_columns\", None)"]},{"cell_type":"code","execution_count":2,"metadata":{"id":"kQUWtqOKQgmH","tags":[]},"outputs":[],"source":["# Data set file path\n","data_url = '/Users/michaelcho/_GoogleDrive/_Classes/CSS 444 A/Final_Project/data/Arrests.csv'"]},{"cell_type":"code","execution_count":3,"metadata":{},"outputs":[],"source":["# Loading dataset into pd DataFrame\n","data = pd.read_csv(data_url)"]},{"cell_type":"markdown","metadata":{},"source":["# Descriptive Analysis"]},{"cell_type":"code","execution_count":4,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["No. of training points: 281255\n","No. of testing points: 70314\n"]},{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>X</th>\n","      <th>Y</th>\n","      <th>RowID</th>\n","      <th>ArrestNumber</th>\n","      <th>Age</th>\n","      <th>Gender</th>\n","      <th>Race</th>\n","      <th>ArrestDateTime</th>\n","      <th>ArrestLocation</th>\n","      <th>IncidentOffence</th>\n","      <th>IncidentLocation</th>\n","      <th>Charge</th>\n","      <th>ChargeDescription</th>\n","      <th>District</th>\n","      <th>Post</th>\n","      <th>Neighborhood</th>\n","      <th>Latitude</th>\n","      <th>Longitude</th>\n","      <th>GeoLocation</th>\n","      <th>Shape</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>171271</th>\n","      <td>1.423933e+06</td>\n","      <td>591809.446923</td>\n","      <td>171272</td>\n","      <td>12505619.0</td>\n","      <td>55.0</td>\n","      <td>M</td>\n","      <td>B</td>\n","      <td>2012/06/29 11:15:00+00</td>\n","      <td>800 FAYETTE ST</td>\n","      <td>87NARCOTICS</td>\n","      <td>800 FAYETTE ST</td>\n","      <td>4 3550</td>\n","      <td>CDS VIOL</td>\n","      <td>Southeast</td>\n","      <td>211</td>\n","      <td>Jonestown</td>\n","      <td>39.2910</td>\n","      <td>-76.6057</td>\n","      <td>(39.291,-76.6057)</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>346948</th>\n","      <td>1.442387e+06</td>\n","      <td>586213.654760</td>\n","      <td>346949</td>\n","      <td>10000971.0</td>\n","      <td>24.0</td>\n","      <td>M</td>\n","      <td>B</td>\n","      <td>2010/01/06 09:15:00+00</td>\n","      <td>1500 CHARLOTTE AV</td>\n","      <td>3KROBB RES. (UA)</td>\n","      <td>1500 CHARLOTTE AV</td>\n","      <td>NaN</td>\n","      <td>Unknown Charge</td>\n","      <td>Southeast</td>\n","      <td>234</td>\n","      <td>Broening Manor</td>\n","      <td>39.2754</td>\n","      <td>-76.5406</td>\n","      <td>(39.2754,-76.5406)</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>10906</th>\n","      <td>1.416643e+06</td>\n","      <td>581908.237423</td>\n","      <td>10907</td>\n","      <td>15188371.0</td>\n","      <td>26.0</td>\n","      <td>M</td>\n","      <td>B</td>\n","      <td>2015/12/16 10:30:00+00</td>\n","      <td>2200 CEDLEY ST</td>\n","      <td>87NARCOTICS</td>\n","      <td>2200 CEDLEY ST</td>\n","      <td>4 3550</td>\n","      <td>CDS</td>\n","      <td>Southern</td>\n","      <td>921</td>\n","      <td>Westport</td>\n","      <td>39.2639</td>\n","      <td>-76.6316</td>\n","      <td>(39.2639,-76.6316)</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>218363</th>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>218364</td>\n","      <td>14076816.0</td>\n","      <td>23.0</td>\n","      <td>F</td>\n","      <td>W</td>\n","      <td>2014/05/12 15:00:00+00</td>\n","      <td>NaN</td>\n","      <td>Unknown Offense</td>\n","      <td>NaN</td>\n","      <td>1 0088</td>\n","      <td>VIOLATION OF PROBATION</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>(,)</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>341962</th>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>341963</td>\n","      <td>22003143.0</td>\n","      <td>30.0</td>\n","      <td>M</td>\n","      <td>B</td>\n","      <td>2022/01/10 07:00:00+00</td>\n","      <td>NaN</td>\n","      <td>Unknown Offense</td>\n","      <td>NaN</td>\n","      <td>1 0077</td>\n","      <td>FAILURE TO APPEAR</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>(,)</td>\n","      <td>NaN</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["                   X              Y   RowID  ArrestNumber   Age Gender Race  \\\n","171271  1.423933e+06  591809.446923  171272    12505619.0  55.0      M    B   \n","346948  1.442387e+06  586213.654760  346949    10000971.0  24.0      M    B   \n","10906   1.416643e+06  581908.237423   10907    15188371.0  26.0      M    B   \n","218363           NaN            NaN  218364    14076816.0  23.0      F    W   \n","341962           NaN            NaN  341963    22003143.0  30.0      M    B   \n","\n","                ArrestDateTime     ArrestLocation         IncidentOffence  \\\n","171271  2012/06/29 11:15:00+00     800 FAYETTE ST  87NARCOTICS              \n","346948  2010/01/06 09:15:00+00  1500 CHARLOTTE AV  3KROBB RES. (UA)         \n","10906   2015/12/16 10:30:00+00     2200 CEDLEY ST  87NARCOTICS              \n","218363  2014/05/12 15:00:00+00                NaN         Unknown Offense   \n","341962  2022/01/10 07:00:00+00                NaN         Unknown Offense   \n","\n","         IncidentLocation  Charge       ChargeDescription   District Post  \\\n","171271     800 FAYETTE ST  4 3550                CDS VIOL  Southeast  211   \n","346948  1500 CHARLOTTE AV     NaN          Unknown Charge  Southeast  234   \n","10906      2200 CEDLEY ST  4 3550                     CDS   Southern  921   \n","218363                NaN  1 0088  VIOLATION OF PROBATION        NaN  NaN   \n","341962                NaN  1 0077       FAILURE TO APPEAR        NaN  NaN   \n","\n","          Neighborhood  Latitude  Longitude         GeoLocation  Shape  \n","171271       Jonestown   39.2910   -76.6057   (39.291,-76.6057)    NaN  \n","346948  Broening Manor   39.2754   -76.5406  (39.2754,-76.5406)    NaN  \n","10906         Westport   39.2639   -76.6316  (39.2639,-76.6316)    NaN  \n","218363             NaN       NaN        NaN                 (,)    NaN  \n","341962             NaN       NaN        NaN                 (,)    NaN  "]},"execution_count":4,"metadata":{},"output_type":"execute_result"}],"source":["train_data = data.sample(frac=0.8, random_state=25)\n","test_data = data.drop(train_data.index)\n","\n","num_train = len(train_data)\n","\n","print(f\"No. of training points: {train_data.shape[0]}\")\n","print(f\"No. of testing points: {test_data.shape[0]}\")\n","\n","original = pd.concat([train_data, test_data])\n","original = original[original['Gender'].notna()]\n","\n","original.head()"]},{"cell_type":"code","execution_count":5,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Total Female: 65151\n","Total Male: 286379\n","Percentage Female: 0.18533553324040622\n","Percentage Male: 0.8146644667595938\n"]}],"source":["print(\"Total Female: \" + str(original['Gender'].value_counts().F))\n","print(\"Total Male: \" + str(original['Gender'].value_counts().M))\n","\n","print(\"Percentage Female: \" + str((original['Gender'].value_counts().F/len(original))))\n","print(\"Percentage Male: \" + str((original['Gender'].value_counts().M/len(original))))"]},{"cell_type":"code","execution_count":6,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["290485\n","52200\n"]},{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>F</th>\n","      <th>M</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>Black</th>\n","      <td>0.162604</td>\n","      <td>0.837396</td>\n","    </tr>\n","    <tr>\n","      <th>White</th>\n","      <td>0.320613</td>\n","      <td>0.679387</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["              F         M\n","Black  0.162604  0.837396\n","White  0.320613  0.679387"]},"execution_count":6,"metadata":{},"output_type":"execute_result"}],"source":["black = original[original['Race']=='B'] #subgrouping\n","white = original[original['Race']=='W']\n","total_black = len(original[original['Race']=='B'])\n","total_white = len(original[original['Race']=='W'])\n","\n","print(total_black)\n","print(total_white)\n","\n","\n","black_female = len(black[(black.Gender == 'F') | (black.Gender == 'F')])/total_black\n","black_male = len(black[(black.Gender == 'M') | (black.Gender == 'M')])/total_black\n","\n","white_female = len(white[(white.Gender == 'F') | (white.Gender == 'F')])/total_white\n","white_male = len(white[(white.Gender == 'M') | (white.Gender == 'M')])/total_white\n","\n","target_percent = pd.DataFrame({'F': [black_female, white_female], 'M': [black_male, white_male]}, index=['Black', 'White'])\n","target_percent"]},{"cell_type":"markdown","metadata":{},"source":["# Logistic Regression"]},{"cell_type":"code","execution_count":7,"metadata":{},"outputs":[],"source":["# Creating a Binary Target Variable\n","# In this case, whether the arrested person is male\n","is_male = original['Gender']\n","is_male = is_male.replace('M', 1).replace('F', 0)"]},{"cell_type":"code","execution_count":8,"metadata":{},"outputs":[{"data":{"text/plain":["171271    B\n","346948    B\n","10906     B\n","218363    W\n","341962    B\n","Name: Race, dtype: object"]},"execution_count":8,"metadata":{},"output_type":"execute_result"}],"source":["# Using race as the only feature\n","data_race = original['Race']\n","data_race.head()"]},{"cell_type":"code","execution_count":9,"metadata":{},"outputs":[],"source":["# Function for normalizing data\n","def data_transform(df):\n","    \"\"\"Normalize features.\"\"\"\n","    binary_data = pd.get_dummies(df)\n","    feature_cols = binary_data[binary_data.columns[:]]\n","    scaler = preprocessing.StandardScaler()\n","    data = pd.DataFrame(scaler.fit_transform(feature_cols), columns=feature_cols.columns)\n","    return data\n","\n","#see here for transformation : https://developers.google.com/machine-learning/data-prep/transform/normalization"]},{"cell_type":"code","execution_count":10,"metadata":{},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Asian</th>\n","      <th>Black</th>\n","      <th>Hispanic</th>\n","      <th>NativeAmerican</th>\n","      <th>Unknown</th>\n","      <th>White</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>-0.051614</td>\n","      <td>0.458420</td>\n","      <td>-0.001687</td>\n","      <td>-0.048531</td>\n","      <td>-0.14341</td>\n","      <td>-0.417600</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>-0.051614</td>\n","      <td>0.458420</td>\n","      <td>-0.001687</td>\n","      <td>-0.048531</td>\n","      <td>-0.14341</td>\n","      <td>-0.417600</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>-0.051614</td>\n","      <td>0.458420</td>\n","      <td>-0.001687</td>\n","      <td>-0.048531</td>\n","      <td>-0.14341</td>\n","      <td>-0.417600</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>-0.051614</td>\n","      <td>-2.181408</td>\n","      <td>-0.001687</td>\n","      <td>-0.048531</td>\n","      <td>-0.14341</td>\n","      <td>2.394638</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>-0.051614</td>\n","      <td>0.458420</td>\n","      <td>-0.001687</td>\n","      <td>-0.048531</td>\n","      <td>-0.14341</td>\n","      <td>-0.417600</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["      Asian     Black  Hispanic  NativeAmerican  Unknown     White\n","0 -0.051614  0.458420 -0.001687       -0.048531 -0.14341 -0.417600\n","1 -0.051614  0.458420 -0.001687       -0.048531 -0.14341 -0.417600\n","2 -0.051614  0.458420 -0.001687       -0.048531 -0.14341 -0.417600\n","3 -0.051614 -2.181408 -0.001687       -0.048531 -0.14341  2.394638\n","4 -0.051614  0.458420 -0.001687       -0.048531 -0.14341 -0.417600"]},"execution_count":10,"metadata":{},"output_type":"execute_result"}],"source":["# Data normalized\n","data = data_transform(data_race)\n","\n","# Labels changed\n","data = data.rename(columns={\"A\": \"Asian\", \"B\": \"Black\", \"H\": \"Hispanic\", \"I\": \"NativeAmerican\", \"U\": \"Unknown\", \"W\": \"White\"})\n","data.head()"]},{"cell_type":"code","execution_count":12,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["[[ 0.00682599  0.1366492   0.00974089  0.02142998  0.09607215 -0.18749153]]\n"]}],"source":["# Logistic Regression Functions\n","cls_male = linear_model.LogisticRegression()\n","cls_male.fit(data,is_male)\n","\n","# Printing coefficients for race\n","print(cls_male.coef_)"]},{"cell_type":"code","execution_count":14,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["0.18533553324040622\n"]}],"source":["from sklearn.metrics import mean_squared_error\n","res_male=cls_male.predict(data)\n","\n","# Standard error measurement\n","print(mean_squared_error(is_male,res_male))"]},{"cell_type":"markdown","metadata":{},"source":["# Independence"]},{"cell_type":"markdown","metadata":{},"source":["# Seperation"]},{"cell_type":"markdown","metadata":{},"source":["### Data Analysis"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["original.info()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["original.isnull().sum()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["original.describe()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["feature = {\n","            \"Row ID\" : original['RowID'],\n","            \"Age\" : original['Age'], \n","            \"Gender\" : original['Gender'],\n","            \"Race\" : original['Race'],\n","            \"Arrest_Time\" : original['ArrestDateTime'],\n","            \"Incident_Offence\" : original['IncidentOffence'],\n","            \"Charge_Description\" : original['ChargeDescription']\n","          }\n","marginal = pd.DataFrame(data=feature, columns=['Row ID', 'Age', 'Gender', 'Race', 'Arrest_Time', 'Incident_Offence', 'Charge_Description'])\n","marginal.head()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["#Prints information of all columns:\n","print(\"Marginal Data information:\")\n","marginal.info()\n","# Number of Nan data\n","print(\"\\n\\nMarginal Data Size: \", len(marginal))\n","print(\"\\n\\nNumber of nan data: \")\n","marginal.isnull().sum()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["#Drop rows with Nan value\n","marginal.dropna(subset = ['Age'], inplace=True)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["print(\"Marginal data size after drop rows with Nan value: \", len(marginal))\n","print(\"\\nNumber of nan data: \")\n","# Number of Nan data\n","marginal.isnull().sum()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["marginal.head()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["marginal.describe()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Subgroup with age 15–19, 20–29, 30–39, 40–49, and 50+\n","age_group = marginal['Age']\n","\n","teen = marginal[age_group.between(15,19)]\n","twenties = marginal[age_group.between(20,29)]\n","thirties = marginal[age_group.between(30,39)]\n","forties =marginal[age_group.between(40,49)]\n","fifties_more = marginal[age_group >= 50]\n","\n","total_teen = len(teen)\n","total_twenties = len(twenties)\n","total_thirties = len(thirties)\n","total_forties = len(forties)\n","total_fifties_more = len(fifties_more)\n","\n","age_title = '15–19', '20–29', '30–39', '40–49', '50+'\n","age_val = [total_teen, total_twenties, total_thirties, total_forties, total_fifties_more]\n","colors = ['r','c','b','g','y']\n","\n","# Create a pieplot\n","plt.pie(age_val, labels=age_title, colors=colors,\n","        wedgeprops = { 'linewidth' : 2, 'edgecolor' : 'white' },\n","        startangle=0,\n","        explode = (0, 0, 0, 0, 0),\n","        autopct = '%1.2f%%')\n","\n","plt.axis('equal') # Try commenting this out.\n","plt.title('Arrested Age Group')\n","plt.show();"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# get the count of each race in the race category\n","marginal.groupby('Race').count()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Race Pie Chart\n","race_title = 'Asian', 'Black', 'Hispanic', 'Native American', 'White', 'Unknown'\n","race_val = [931, 290403, 1, 826, 7079, 52193]\n","colors = ['c','y', 'k','r','g','m']\n","\n","# Create a pieplot\n","plt.pie(race_val, labels=race_title, labeldistance=1.15, \n","        wedgeprops = { 'linewidth' : 0.4, 'edgecolor' : 'white' },\n","        colors=colors,\n","        startangle=90,\n","        explode = (0, 0, 0.2, 0.4, 0.6, 0),\n","        autopct = '%1.2f%%')\n","\n","plt.axis('equal')\n","plt.title('Arrested Race Group\\n')\n","\n","plt.show();"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Subgroup Gender\n","gender_group = marginal['Gender']\n","female = marginal[gender_group == 'F']\n","male = marginal[gender_group =='M']\n","\n","# Gender Pie Chart\n","gender_title = 'Male', 'Female'\n","gender_val = [len(male), len(female)]\n","colors = ['b','m']\n","\n","plt.pie(gender_val, labels=gender_title, labeldistance=1.15,\n","        wedgeprops = { 'linewidth' : 2, 'edgecolor' : 'white' },\n","        colors=colors,\n","        startangle=90,\n","        explode = (0, 0),\n","        autopct = '%1.2f%%')\n","\n","plt.axis('equal')\n","plt.title('Arrested Gender Group\\n')\n","\n","plt.show();"]},{"cell_type":"markdown","metadata":{},"source":["### ROC Curve"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# target\n","is_male = marginal['Gender']\n","is_male = is_male.replace('M', 1).replace('F', 0)\n","\n","# drop redundent column\n","del marginal[\"Gender\"]\n","marginal.head()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# normalize the data\n","data = marginal\n","from sklearn import preprocessing\n","le = preprocessing.LabelEncoder()\n","for column_name in data.columns:\n","  if data[column_name].dtype == object:\n","    data[column_name] = le.fit_transform(data[column_name])\n","  else:\n","    pass\n","data.head()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# split data to with train data and test data with ratio 0.7:0.3\n","from sklearn.model_selection import train_test_split\n","train_data, test_data, train_label, test_label = train_test_split(data, is_male, test_size = 0.3)\n","train_num = len(train_data)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["log_re = LogisticRegression();\n","log_re.fit(train_data, train_label)\n","# keep probabilities for the positive outcome only\n","score = log_re.predict_proba(test_data)[:,1] \n","log_re.coef_"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Redo the marginal data frame since the original marginal is overwrite with the normalized data\n","marginal = pd.DataFrame(data=feature, columns=['Row ID', 'Age', 'Gender', 'Race', 'Arrest_Time', 'Incident_Offence', 'Charge_Description'])\n","# drops rows with Nan value based on the Age column\n","marginal.dropna(subset = ['Age'], inplace=True)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["original_test = marginal[train_num:]\n","dataFrame = {'target': test_label.values,\n","             'score':  score,           \n","             'race': original_test['Race']\n","             }\n","marginal2 = pd.DataFrame(data=dataFrame, columns=['target', 'score', 'race'])\n","marginal2.head() "]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# ploting ROC Curves\n","white = marginal2[marginal2['race'] == 'W']\n","fpr_white, tpr_white, _ = metrics.roc_curve(white['target'], white['score'])\n","auc_white = auc(fpr_white,tpr_white)\n","\n","black = marginal2[marginal2['race'] == 'B']\n","fpr_black, tpr_black, _ = metrics.roc_curve(black['target'], black['score'])\n","auc_black = auc(fpr_black,tpr_black)\n","\n","nativeAm = marginal2[marginal2['race'] == 'I']\n","fpr_na, tpr_na, _ = metrics.roc_curve(nativeAm['target'], nativeAm['score'])\n","auc_na = auc(fpr_na,tpr_na)\n","\n","asian = marginal2[marginal2['race'] == 'A']\n","fpr_asian, tpr_asian, _ = metrics.roc_curve(asian['target'], asian['score'])\n","auc_asian = auc(fpr_asian,tpr_asian)\n","\n","\n","plt.figure(1, figsize=(6, 5))\n","plt.title('ROC curves by Race')\n","plt.xlabel('False positive rate')\n","plt.ylabel('True positive rate')\n","plt.plot(fpr_white, tpr_white, label='white (AUC = %0.3f)' % auc_white, linestyle='-', color='g')\n","plt.plot(fpr_black, tpr_black, label='black (AUC = %0.3f)' % auc_black, linestyle=':', color='r')\n","plt.plot(fpr_na, tpr_na, label='Native American (AUC = %0.3f)' % auc_na, linestyle='--', color='m')\n","plt.plot(fpr_asian, tpr_asian, label='Asian (AUC = %0.3f)' % auc_asian, linestyle='-.', color='b')\n","\n","\n","plt.legend()\n","plt.show()"]},{"cell_type":"markdown","metadata":{},"source":["# Relaxation of Seperation"]},{"cell_type":"markdown","metadata":{},"source":["# Equal Opportunity"]},{"cell_type":"markdown","metadata":{},"source":[]}],"metadata":{"colab":{"name":"Copy of final_notebook.ipynb","provenance":[]},"interpreter":{"hash":"a9d0a0dcbf53ff149347ec2ac99c499aef6060157925b8e19140738fbce6ee13"},"kernelspec":{"display_name":"Python 3.10.2 ('venv': venv)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.2"},"orig_nbformat":4},"nbformat":4,"nbformat_minor":0}
